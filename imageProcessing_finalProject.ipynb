{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "imageProcessing finalProject",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gKRfGFFfXeN2"
      },
      "source": [
        "Relevant imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K-eDwGxce1RK"
      },
      "source": [
        "import random\n",
        "from math import floor, ceil\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.utils import data\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torchvision.transforms import Compose, Resize, ToTensor"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aaSnLvk9eHdL",
        "outputId": "4ffdf129-b344-42a2-a789-9d674fad443d"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6eBGyLuAXJBt"
      },
      "source": [
        "Reading data and split to train and validation sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "abV7KdnHrh6f"
      },
      "source": [
        "# Read the data and compress it to 128*128 pixels\n",
        "# Train set is a type of data set that contains all the images from both classifications\n",
        "# with their labels\n",
        "trainDataSet = ImageFolder(\"/content/drive/MyDrive/Colab Notebooks/dataCovid19\", transform=Compose([Resize((128, 128)), ToTensor()]))\n",
        "# Split to train and validation in ratio of 80:20\n",
        "trainDataSet, validationDataSet = torch.utils.data.random_split(trainDataSet,\n",
        "                                  [int(floor(len(trainDataSet)*0.80)), int(ceil(len(trainDataSet)*0.20))])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4R9UfmwiXvPa"
      },
      "source": [
        "Mixup Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J4g-1e7oXZPA"
      },
      "source": [
        "# Get a data set and return a mixup image\n",
        "class MixUpData(Dataset):\n",
        "    def __init__(self, data_set):\n",
        "        self.data_set = data_set\n",
        "        self.mixup = []\n",
        "        # Create the images of the mixup\n",
        "        for i in range(8192):\n",
        "            # Choose indexes of two randomally images\n",
        "            idx1 = random.randint(0, len(self.data_set) - 1)\n",
        "            idx2 = random.randint(0, len(self.data_set) - 1)\n",
        "\n",
        "            # Get the vector of the images in the indexes\n",
        "            image1 = self.data_set[idx1][0]\n",
        "            image2 = self.data_set[idx2][0]\n",
        "\n",
        "            # Create vectors for the labels (one hot vector)\n",
        "            label1 = torch.zeros(2)\n",
        "            label1[self.data_set[idx1][1]] = 1.\n",
        "            label2 = torch.zeros(2)\n",
        "            label2[self.data_set[idx2][1]] = 1.\n",
        "\n",
        "            # Get a randomally lambda in a uniform distribution between 0.1 to 0.3\n",
        "            # to get an extreme values\n",
        "            lamb = np.random.uniform(0.1, 0.3, [1])[0]\n",
        "\n",
        "            # Formula of mixup to get the new image and label\n",
        "            mixImage = lamb * image1 + (1 - lamb) * image2\n",
        "            mixLabel = lamb * label1 + (1 - lamb) * label2\n",
        "            self.mixup.append([mixImage, mixLabel])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.mixup)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.mixup[idx][0], self.mixup[idx][1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lHlnAY32X6Fu"
      },
      "source": [
        "Preparing the data for the learing models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RQR23uk4X7Fc"
      },
      "source": [
        "trainDataSet = MixUpData(trainDataSet)\n",
        "# Prepare the train and validation sets to be an input for the NN\n",
        "trainLoader = torch.utils.data.DataLoader(trainDataSet, batch_size=64, shuffle=True)\n",
        "validationLoader = torch.utils.data.DataLoader(validationDataSet, batch_size=64, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WPOpjMJDYMW1"
      },
      "source": [
        "Deep learning model - CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p8zxVuQEYRs8"
      },
      "source": [
        "# CNN parameters\n",
        "cls = [16, 32, 64, 128]  # convolution layers size\n",
        "krn = 5  # kernel size\n",
        "std = 1  # stride\n",
        "pad = 2  # num of padding\n",
        "mpk = 2  # maxpool kernel\n",
        "mps = 2  # maxpool stride\n",
        "\n",
        "\n",
        "# CNN - deep learning method\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "        self.layer1 = nn.Sequential(\n",
        "            nn.Conv2d(3, cls[0], kernel_size=krn, stride=std, padding=pad),\n",
        "            nn.BatchNorm2d(cls[0]),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=mpk, stride=mps),\n",
        "            nn.Dropout())\n",
        "        self.layer2 = nn.Sequential(\n",
        "            nn.Conv2d(cls[0], cls[1], kernel_size=krn, stride=std, padding=pad),\n",
        "            nn.BatchNorm2d(cls[1]),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=mpk, stride=mps),\n",
        "            nn.Dropout())\n",
        "        self.layer3 = nn.Sequential(\n",
        "            nn.Conv2d(cls[1], cls[2], kernel_size=krn, stride=std, padding=pad),\n",
        "            nn.BatchNorm2d(cls[2]),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=mpk, stride=mps),\n",
        "            nn.Dropout())\n",
        "        self.layer4 = nn.Sequential(\n",
        "            nn.Conv2d(cls[2], cls[3], kernel_size=krn, stride=std, padding=pad),\n",
        "            nn.BatchNorm2d(cls[3]),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=mpk, stride=mps),\n",
        "            nn.Dropout())\n",
        "        self.layer5 = nn.Sequential(\n",
        "            nn.Linear(8192, 1024),\n",
        "            nn.BatchNorm1d(1024),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(1024, 2))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "        x = x.reshape(x.size(0), -1)\n",
        "        x = self.layer5(x)\n",
        "        output = nn.functional.softmax(x, dim=1)\n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qhw5xux4Ycfq"
      },
      "source": [
        "Non deep learning model - Perceptron"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "27EnTgx5YiR9"
      },
      "source": [
        "# Perceptron - non deep learning method\n",
        "class Perceptron(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Perceptron, self).__init__()\n",
        "        self.perceptron = nn.Linear(49152, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.reshape(x.size(0), -1)\n",
        "        x = self.perceptron(x)\n",
        "        output = nn.functional.softmax(x, dim=1)\n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oEgi8R7LYrJ_"
      },
      "source": [
        "Train the models and validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k0g5f5CbYlS0"
      },
      "source": [
        "# Train the model\n",
        "def train(model, optimizer):\n",
        "    model.train()\n",
        "    # Number of epochs\n",
        "    for epoch in range(64):\n",
        "        trainLoss = 0\n",
        "        correctOutput = 0\n",
        "        for batchIdx, (data, labels) in enumerate(trainLoader):\n",
        "            optimizer.zero_grad()\n",
        "            output = model(data)\n",
        "            loss = nn.functional.mse_loss(output, labels)\n",
        "            trainLoss += nn.functional.mse_loss(output, labels).item()\n",
        "            prediction = output.max(1, keepdim=True)[1]\n",
        "            true = labels.max(1, keepdim=True)[1]\n",
        "            correctOutput += prediction.eq(true).cpu().sum()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "\n",
        "# Validation of the model\n",
        "def validation(model):\n",
        "    model.eval()\n",
        "    validLoss = 0\n",
        "    correctOutput = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in validationLoader:\n",
        "            output = model(data)\n",
        "            labelAsOneHot = []\n",
        "            for i in target:\n",
        "                tmp = [0, 0]\n",
        "                tmp[i] = 1.\n",
        "                labelAsOneHot.append(tmp)\n",
        "            labelAsOneHot = torch.from_numpy(np.array(labelAsOneHot))\n",
        "            validLoss += nn.functional.mse_loss(output, labelAsOneHot).item()\n",
        "            prediction = output.max(1, keepdim=True)[1]\n",
        "            correctOutput += prediction.eq(target.view_as(prediction)).cpu().sum()\n",
        "    validLoss /= len(validationLoader.dataset)\n",
        "    #print(\"Validation - Avg Loss: \" + str(validLoss) + \", Accuracy: \" + str(int(100. * correctOutput / len(validationLoader.dataset))) + \"%\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prfLzxmFYwxw"
      },
      "source": [
        "Activate the models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hyrewUxrYwEk"
      },
      "source": [
        "# Activate the perceptron model\n",
        "model1 = Perceptron()\n",
        "optimizer1 = optim.SGD(model1.parameters(), lr=0.1)\n",
        "train(model1, optimizer1)\n",
        "validation(model1)\n",
        "\n",
        "# Activate the CNN model\n",
        "model2 = CNN()\n",
        "optimizer2 = optim.SGD(model2.parameters(), lr=0.1)\n",
        "train(model2, optimizer2)\n",
        "validation(model2)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}